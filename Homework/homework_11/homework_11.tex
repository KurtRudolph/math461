\documentclass{article}
\usepackage{hw_style}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{mathtools}

% Homework Specific Information
\newcommand{\hmwkTitle}{Homework \#11}
\newcommand{\hmwkDueDate}{Friday, July 20, 2012}
\newcommand{\hmwkAuthorName}{Kurt Rudolph}%Name:
\newcommand{\hmwkNetID}{rudolph9}%your netid
\newcommand{\hmwkNotes}{}%I worked with...

\newcommand{\hmwkSubTitle}{}
\newcommand{\hmwkClass}{Math 461}
\newcommand{\hmwkClassTime}{}
\newcommand{\hmwkClassInstructor}{Kenneth B. Stolarsky}

\begin{document}
\begin{spacing}{1.1}
\maketitle
%=============================p.375 #36==========================% 
\newpage
\begin{homeworkProblem}
  {\bf Chapter 7, Exercise 36}\\
  Let $X$ be the number of 1's and Y the number of 2's that occur in 
  $n$ rolls of a fair die. Compute $Cov( X, Y)$.
  \begin{homeworkSection}{Definitions}
  {\bf Covariance}\\
    \begin{align*}
      Cov( X, Y) &= E[ (X - E[X]) (Y - E[Y])] \\
      &= E[ XY] - E[ X]E[ Y]
    \end{align*}
  \end{homeworkSection}
  \begin{homeworkSection}{Solution}
    \begin{align*}
      \text{Let}\\
        X &= \sum_{i = 1}^n X_i\\
        Y &= \sum_{j = 1}^n Y_j\\
      \text{hence,}\\
        Cov( X, Y) &= Cov\left(\sum_{i = 1}^n X_i, \sum_{j = 1}^n Y_j\right)\\
        &= \sum_{i = 1}^n\sum_{j = 1}^n Cov(X_i, Y_i)\\
      \text{since $E[XY] = 0$}\\
        &= \sum_{i = 1}^n -\frac{ 1}{36}\\
        &= -\frac{ n}{ 36}
    \end{align*}
  \end{homeworkSection}
\end{homeworkProblem}

%=============================p.375 #37==========================% 
\newpage
\begin{homeworkProblem}
  {\bf Chapter 7, Exercise 37}\\
  A die is rolled twice. Let $X$ equal the sum of the outcomes, and 
  let $Y$ equal the first outcome minus the second. Compute $Cov(X, Y)$.
  \begin{homeworkSection}{Solution}
    \begin{align*}
      \text{Given}\\
        X &= E_1 + E_2\\
        Y &= E_1 - E_2\\
      \text{Since}\\
        E[XY] &= E[X][Y]\\
      \text{Therefore}\\
        Cov(X, Y) &= 0
    \end{align*}
  \end{homeworkSection}
\end{homeworkProblem}

%=============================p.376 #40==========================% 
\newpage
\begin{homeworkProblem}
  {\bf Chapter 7, Exercise 40}\\
  The joint density function of $X$ and $Y$ is given by
    \[f( x, y) = \frac{ 1}{ y} e^{-(y + x / y)}, x > 0, y > 0\]
  Find $E[ X], E[ Y]$, and show $Cov( X, Y) = 1$.
  \begin{homeworkSection}{Solution}
    \begin{align*}
      E[ X] &= \int\limits_{x = 0}^\infty x \int_{y = 0}^\infty f( x, y) dy dx\\
      &= \int\limits_{x = 0}^\infty \int_{y = 0}^\infty \frac{ x}{ y} e^{-(y + x / y)} dy dx\\
      &= \int\limits_{y = 0}^\infty \int_{x = 0}^\infty \frac{ x}{ y} e^{-(y + x / y)} dx dy\\
      \text{Let $ v = \frac{ x}{ y}$ where $dv = \frac{ dx}{ y}$}\\
      &= \int\limits_{y = 0}^\infty \int_{v = 0}^\infty v e^{-(y + v)} y dv dy\\
      &= \int\limits_{y = 0}^\infty \int_{v = 0}^\infty v e^{-y} e^{-v} y dv dy\\
      &= \int\limits_{y = 0}^\infty e^{-y} y dy\\
      &= 1
    \end{align*}
    \begin{align*}
      E[ Y] &= \int\limits_{y = 0}^\infty y \int_{x = 0}^\infty y f( x, y) dx dy\\
      &= \int\limits_{y = 0}^\infty \int_{x = 0}^\infty e^{-(y + x / y)} dx dy\\
      \text{Let $ v = \frac{ x}{ y}$ where $dv = \frac{ dx}{ y}$}\\
      &= \int\limits_{y = 0}^\infty \int_{v = 0}^\infty e^{-(y + v)} y dv dy\\
      &= \int\limits_{y = 0}^\infty \int_{v = 0}^\infty e^{-y} e^{-v} y dv dy\\
      &= \int\limits_{y = 0}^\infty  e^{-y} y dy\\
      &= 1
    \end{align*}
    \begin{align*}
      E[ XY] &= \int\limits_{y = 0}^\infty y \int_{x = 0}^\infty xy f( x, y) dx dy\\
      &= \int\limits_{y = 0}^\infty \int_{x = 0}^\infty xe^{-(y + x / y)} dx dy\\
      \text{Let $ v = \frac{ x}{ y}$ where $dv = \frac{ dx}{ y}$}\\
      &= \int\limits_{y = 0}^\infty \int_{v = 0}^\infty v e^{-(y + v)} y^2 dv dy\\
      &= \int\limits_{y = 0}^\infty \int_{v = 0}^\infty e^{-y} e^{-v} y^2 dv dy\\
      &= \int\limits_{y = 0}^\infty  e^{-y} y^2 dy\\
      &= 2
    \end{align*}
    Therefore, \[Cov(X, Y) = E[XY] - E[X]E[Y] = 2 - 1 = 1\]
  \end{homeworkSection}
\end{homeworkProblem}

%=============================p.380 #4==========================% 
\newpage
\begin{homeworkProblem}
  {\bf Chapter 7, Theoretical Exercise 4}\\
  Let $X$ be a random variable having finite expectation $\mu$ and 
  variance $\sigma^2$, and let $g(\cdot)$ be a twice differentiable function. 
  Show that
  \[E[ g( X)] \approx g( \mu) + \frac{ g''(\mu)}{ 2} \sigma^2\]
  \emph{Hint}: Expand $g(\cdot)$ in a Taylor series about $\mu$. Use the 
  first three terms and ignore the remainder.
  \begin{homeworkSection}{Solution}
    Following the hint and expanding the \emph{Taylor Series} about $\mu$, we find
    \begin{align*}
      E[g(X)]
      \approx & g(\mu) + \frac{ g''(\mu)}{ 2} (X - \mu)^2\\
      \approx & g(\mu) + \frac{ g''(\mu)}{ 2} (X - E[X])^2\\
      \approx & g(\mu) + \frac{ g''(\mu)}{ 2} \sigma^2\\
    \end{align*}
    
  \end{homeworkSection}
\end{homeworkProblem}
  
\end{spacing}
\end{document}

\begin{comment}%==========================================================
%=============================Problemi==========================% 
\newpage
\begin{homeworkProblem}
  
  \begin{homeworkSection}{Solution}
    
  \end{homeworkSection}
\end{homeworkProblem}
%=============================Problemi==========================% 
\newpage
\begin{homeworkProblem}
  
  \begin{enumerate}[(a)]
    \item 
      \begin{homeworkSection}{Solution}
    
      \end{homeworkSection}
  \end{enumerate}
\end{homeworkProblem}
